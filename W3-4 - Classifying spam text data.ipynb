{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying *text* data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this point, we have *not* been classifying text data. The question is: If we are working with text, what should the features be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set: SMS Spam Collection\n",
    "\n",
    "Info is [here](https://github.com/trevordmiller/practice-python-data-science/tree/master/Python-Data-Science-and-Machine-Learning-Bootcamp/Machine%20Learning%20Sections/Natural-Language-Processing/smsspamcollection)\n",
    "\n",
    "> The SMS Spam Collection v.1 (hereafter the corpus) is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we'll read it in, and take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, random\n",
    "csvfile = open('corpora/spam_collection.csv')\n",
    "dlist = list(csv.DictReader(csvfile, delimiter=\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlist[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we have to tokenize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing this turns out to be a little of a pain. \n",
    "Since this was text posted online, people used all sorts of abbreviations, and contractions were important.\n",
    "\n",
    "We'll use a special-purpose tokenizer I created for some twitter data. This uses some techniques that will learn about in a later week. Basically, it's the result of a lot of fiddling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruces_twitter_tokenizer(text):\n",
    "    import re\n",
    "    \n",
    "    def is_contraction(the_text):\n",
    "        contraction_patterns = re.compile(r\"(?i)(.)('ll|'re|'ve|n't|'s|'m|'d)\\b\")\n",
    "        return contraction_patterns.search(the_text)\n",
    "    \n",
    "    punctuation_class = r\"([\\.\\-\\/&\\\";:\\(\\)\\?\\!\\]\\[\\{\\}\\*#])\"\n",
    "    \n",
    "    # eliminate_urls\n",
    "    text = re.sub(r\"http\\S*\", \"\", text)\n",
    "    \n",
    "    # elimintate hashtags, user mentionds\n",
    "    text = re.sub(r\"#\\S*\", \"\", text)\n",
    "    text = re.sub(r\"@\\S*\", \"\", text)\n",
    "    \n",
    "    # Separate most punctuation at end of words\n",
    "\n",
    "    text = re.sub(r\"(\\w)\" + punctuation_class, r'\\1 \\2 ', text)\n",
    "    \n",
    "    # Separate most punctuation at start of words\n",
    "    text = re.sub(punctuation_class + r\"(\\w)\", r'\\1 \\2', text)\n",
    "    \n",
    "    # Separate punctuation from other punctuation\n",
    "    text = re.sub(punctuation_class + punctuation_class, r'\\1 \\2 ', text)\n",
    "    \n",
    "    # Put spaces between + and = signs and digits. Also %s that follow a digit, $s that come before a digit\n",
    "    text = re.sub(r\"(\\d)([+=%])\", r'\\1 \\2 ', text)\n",
    "    text = re.sub(r\"([\\$+=])(\\d)\", r'\\1 \\2', text)\n",
    "    \n",
    "    # Separate commas if they're followed by space.\n",
    "    # (E.g., don't separate 2,500)\n",
    "    text = re.sub(r\"(,\\s)\", r' \\1', text)\n",
    "    \n",
    "    #when we have two double quotes make it 1.\n",
    "    #\n",
    "    text = re.sub(\"\\\"\\\"\", \"\\\"\", text)\n",
    "\n",
    "    # Separate leading and trailing single and double quotes .\n",
    "    text = re.sub(r\"(\\'\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\')\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(\\\"\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\\")\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(^\\\")\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(^\\')\", r'\\1 ', text)\n",
    "    text = re.sub(r\"('\\'$)\", r' \\1', text)\n",
    "    text = re.sub(r\"('\\\"$)\", r' \\1', text)\n",
    "\n",
    "    #Separate parentheses where appropriate\n",
    "    text = re.sub(r\"(\\)\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\()\", r'\\1 ', text)\n",
    "\n",
    "    # Separate periods that come before newline or end of string.\n",
    "    text = re.sub('\\. *(\\n|$)', ' . ', text)\n",
    "    \n",
    "    # separate single quotes in the middle of words\n",
    "    # text = re.sub(r\"(\\w)(\\')(\\w)\", r'\\1 \\2 \\3', text)\n",
    "    \n",
    "    # separate out 's at the end of words\n",
    "    text = re.sub(r\"(\\w)(\\'s)(\\s)\", r\"\\1 s \", text)\n",
    "    split_text = text.split()\n",
    "    \n",
    "    return split_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_list = []\n",
    "for n, row in enumerate(dlist):\n",
    "    if row[\"text\"] is None:\n",
    "        print(n)\n",
    "        break\n",
    "    new_row = [bruces_twitter_tokenizer(row[\"text\"].lower()), row[\"spam\"]]\n",
    "    tokenized_data_list.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting each text item to a featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways we could convert each response to a featureset. To start, we going to use a sort a \"bag-of-words\" representation. We are going to first create vocabulary consisting of some of the most common words. Then each feature will be the presence or absence of each item in our vocabulary. The order of words won't matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build a stop list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lists/stop-words_english_5_en.txt\")\n",
    "stop_list = f.read().split(\"\\n\")\n",
    "stop_list += list('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~â€™')\n",
    "stop_list += list(\"abcdefghijklmnopqrstuvwxyz0123456789\")\n",
    "stop_list = set(stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a frequency distribution, ignoring words on the stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "word_fdist = nltk.FreqDist() # the corpus frequences\n",
    "for row in tokenized_data_list:\n",
    "    for word in row[0]:\n",
    "        if not word in stop_list:\n",
    "            word_fdist[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary will be the 100 most common words remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = word_fdist.most_common(100)\n",
    "vocab = [w[0] for w in mc]\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feature_extractor(the_text, vocab):\n",
    "    features = {}\n",
    "    for word in vocab:\n",
    "        features['contains({})'.format(word)] = (word in the_text)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_feature_extractor(tokenized_data_list[0][0], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_featuresets = []\n",
    "for row in tokenized_data_list:\n",
    "    new_lf = [word_feature_extractor(row[0], vocab), row[1]]\n",
    "    labeled_featuresets.append(new_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(labeled_featuresets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate all of the labeled featuresets into a test set and a training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the labeled_featuresets, everything else is the same as what we did in the previous notebooks, that didn't use text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(.1 * len(labeled_featuresets))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = labeled_featuresets[:test_size]\n",
    "train_set = labeled_featuresets[test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list = [t[1] for t in test_set]\n",
    "guess_list = [spam_classifier.classify(t[0]) for t in test_set]\n",
    "cm = nltk.ConfusionMatrix(gold_list, guess_list)\n",
    "print(cm)\n",
    "print(\"accuracy = \", nltk.classify.accuracy(spam_classifier, test_set))\n",
    "print(\"kappa = \", cohen_kappa_score(gold_list, guess_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_classifier = nltk.DecisionTreeClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list = [t[1] for t in test_set]\n",
    "guess_list = [spam_classifier.classify(t[0]) for t in test_set]\n",
    "cm = nltk.ConfusionMatrix(gold_list, guess_list)\n",
    "print(cm)\n",
    "print(\"accuracy = \", nltk.classify.accuracy(spam_classifier, test_set))\n",
    "print(\"kappa = \", cohen_kappa_score(gold_list, guess_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spam_classifier.pseudocode(depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spam_classifier.pseudocode(depth=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.1630554199219px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
