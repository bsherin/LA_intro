{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vec(v):\n",
    "    return v / np.linalg.norm(v)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using external modules\n",
    "To make our lives simpler, and to make these notebooks a little less crowded, we can stick functions we use frequently into separate modules. (This are files with the extention .py.) Then we can import individual functions from them. Or we can import everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line gives allows you do use everything from utilities as if it were in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import * \n",
    "corpus = load_entire_directory(\"corpora/seasons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you import like this you can access everything from utilities like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utilities\n",
    "corpus = utilities.load_entire_directory(\"corpora/seasons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can import just one thing from a module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import load_entire_directory\n",
    "corpus = load_entire_directory(\"corpora/seasons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can give modules shorthand names for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import load_entire_directory as led\n",
    "corpus = led(\"corpora/seasons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seasons_module import load_seasons_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Seasons Corpus</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seasons_corpus = load_seasons_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['that', 's', 'because', 'of', 'the', 'sun', 'is', 'in', 'the', 'center', 'and', 'the', 'earth', 'moves', 'around', 'the', 'sun', 'and', 'the', 'earth', 'is', 'like', 'at', 'one', 'point', 'in', 'the', 'winter', 'it', 's', 'like', 'farther', 'away', 'from', 'the', 'sun', 'and', 'towards', 'the', 'summer', 'it', 's', 'closer', 'it', 's', 'near', 'towards', 'the', 'sun', 'okay', 'the', 'sun', 's', 'in', 'the', 'middle', 'and', 'the', 'earth', 'kind', 'of', 'orbits', 'around', 'it', 'and', 'like', 'say', 'at', 'one', 'it', 's', 'probably', 'more', 'of', 'an', 'ovally', 'type', 'thing', 'in', 'the', 'winter', 'er', 'probably', 'this', 'will', 'be', 'winter', 'since', 'it', 's', 'further', 'away', 'see', 'that', 's', 'winter', 'would', 'be', 'like', 'the', 'earth', 'orbits', 'around', 'the', 'sun', 'like', 'summer', 'is', 'the', 'closest', 'to', 'the', 'sun', 'spring', 'is', 'kind', 'of', 'a', 'little', 'further', 'away', 'and', 'then', 'like', 'fall', 'is', 'further', 'away', 'then', 'spring', 'but', 'not', 'as', 'far', 'as', 'winter', 'and', 'then', 'winter', 'is', 'the', 'furthest', 'so', 'the', 'sun', \"doesn't\", 'like', 'the', 'flashlight', 'and', 'the', 'bulb', 'it', 'hits', 'summer', 'the', 'lines', 'like', 'fade', 'in', 'they', 'get', 'there', 'closer', 'like', 'quicker', 'and', 'by', 'the', 'time', 'they', 'get', 'there', 'winter', 'it', 'fades', 'and', 'it', 's', 'a', 'lot', 'colder', 'for', 'winter', 'and', 'spring', 'it', 's', 'kind', 'of', 'between', 'the', 'two', 'and', 'same', 'for', 'fall', 'uh', 'kind', 'of', 'like', 'from', 'first', 'and', 'second', 'grade', 'i', 'remember', 'the', 'time', 'that', 'earth', 'orbiting', 'and', 'whatnot', 'mm', 'hmm', 'umm', 'i', 'need', 'another', 'picture', 'yeah', 'that', 'is', 'um', 'ok', 'there', 'is', 'the', 'sun', 'yeah', 'i', 'remember', 'that', 'now', 'cause', 'um', 'it', 's', 'like', 'as', 'the', 'earth', 'is', 'rotating', 'as', 'it', 's', 'orbiting', 'it', 's', 'rotating', 'too', 'i', 'guess', 'i', \"don't\", 'understand', 'it', 'it', 's', 'like', 'spinning', 'cause', 'it', 's', 'going', 'like', 'that', 's', 'how', 'it', 's', 'day', 'and', 'night', 'too', 'yeah', 'so', 'yeah', 'i', 'guess', 'i', 'really', \"don't\", 'understand', 'it', 'that', 'much', 'yeah', 'i', 'have', 'heard', 'that', 'cause', 'i', 'was', 'supposed', 'to', 'go', 'to', 'australia', 'this', 'summer', 'but', 'it', 'was', 'going', 'to', 'be', 'winter', 'when', 'i', 'was', 'going', 'but', 'their', 'winters', 'are', 'really', 'warm', 'i', 'gue', 'yeah', 'kinda', 'yeah', 'because', 'yeah', 'i', 'rethought', 'that', 'and', 'it', 'looks', 'really', 'stupid', 'because', 'summer', 'is', 'really', 'close', 'but', 'how', 'could', 'you', 'winter', 'on', 'the', 'other', 'side', 'how', 'could', 'it', 'be', 'winter', 'on', 'the', 'other', 'side', 'if', 'it', 's', 'really', 'close', 'here', 'and', 'how', 'could', 'it', 'be', 'really', 'warm', 'if', 'this', 'winter', 'earth', 'is', 'really', 'far', 'away', 'i', \"don't\", 'know', 'that', 'looks', 'really', 'dumb', 'to', 'me', 'now', 'alright'], 'cf']\n"
     ]
    }
   ],
   "source": [
    "print(seasons_corpus[\"angelapre\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vanessapre': 'cf', '-vanessapost': 'none', '-williampost': 'none', 'lesliepre': 'side', 'zeldapre': 'tilt', 'williampre': 'none', '-jacobpost': 'none', '-marthapost': 'tilt', '-robbiepost': 'side', 'kurtpre': 'side', 'angelapre': 'cf', 'ftcandice': 'none', '-bethpost': 'tilt', 'ftharmony': 'none', '-alipost': 'none', 'ftcaitlin': 'tilt', 'denisepre': 'side', 'randypre': 'cf', 'ovadyapre': 'cf', '-kurtpost': 'none', 'ftblake': 'tilt', 'alipre': 'side', 'jacobpre': 'side', 'ftsamantha': 'side', 'kimberleypre': 'none', '-ovadyapost': 'cf', '-stanpost': 'tilt', 'ftcassandra': 'side', 'ftlibby': 'side', '-randypost': 'side', '-sandrapost': 'tilt', 'ftlisa': 'tilt', 'ftmarcus': 'cf', 'kellypre': 'none', 'deidrapre': 'side', 'ftalex': 'none', 'ftelesha': 'tilt', 'bethpre': 'tilt', '-amandapost': 'tilt', 'jillpre': 'cf', '-kimberleypost': 'none', '-denisepost': 'side', 'robbiepre': 'side', 'edgarpre': 'none', 'ftmason': 'none', 'ftholly': 'side', 'markpre': 'tilt', 'amandapre': 'tilt', '-edgarpost': 'cf', 'richardpre': 'none', '-markpost': 'tilt', 'ftkerri': 'none', 'ftsophia': 'none', 'ftcaden': 'tilt'}\n"
     ]
    }
   ],
   "source": [
    "d = {}\n",
    "for name, val in seasons_corpus.items():\n",
    "    d[name] = val[1]\n",
    "print(str(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the vocabulary**\n",
    "\n",
    "This is every unique word in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_vocab = set([])\n",
    "for fname in seasons_corpus.keys():\n",
    "    set_vocab = set_vocab.union(set(seasons_corpus[fname][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a stop list. Then remove all of these words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lists/seasons_stop_list.txt\")\n",
    "stop_list = set(f.read().split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_vocab = set(sorted([w for w in list(set_vocab) if w not in stop_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pruned_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the document vector for each document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_vectors = {}\n",
    "for fname in seasons_corpus.keys():\n",
    "    doc_vectors[fname] = np.array([seasons_corpus[fname][0].count(word) for word in pruned_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 2 0 0 0 0 3 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 0 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1\n",
      " 0 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 7 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(doc_vectors[\"angelapre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the vectors\n",
    "for fname, vec in doc_vectors.items():\n",
    "    doc_vectors[fname] = norm_vec(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.085 0.    0.    0.085 0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.17  0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.17  0.    0.    0.\n",
      " 0.    0.255 0.    0.    0.    0.    0.17  0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.085 0.17  0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.085 0.\n",
      " 0.    0.    0.    0.085 0.    0.    0.    0.    0.    0.085 0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.085\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.085 0.    0.    0.    0.17  0.    0.    0.    0.    0.    0.    0.\n",
      " 0.085 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.085 0.085 0.    0.17  0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.085 0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.17  0.    0.    0.\n",
      " 0.085 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.085 0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.085 0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.17  0.    0.    0.085 0.    0.    0.    0.\n",
      " 0.    0.    0.17  0.17  0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.085 0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.085 0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.085\n",
      " 0.    0.    0.    0.    0.    0.085 0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.17  0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.085 0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.085 0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.085 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.085 0.    0.    0.    0.085 0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.085 0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.085 0.085 0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.17  0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.596 0.\n",
      " 0.    0.    0.085 0.    0.085 0.    0.    0.    0.    0.    0.    0.\n",
      " 0.085 0.    0.    0.085]\n"
     ]
    }
   ],
   "source": [
    "print(doc_vectors[\"angelapre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare some pairs of students**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_students(s1, s2):\n",
    "    return round(dot(doc_vectors[s1], doc_vectors[s2]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.301"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_students('alipre', 'jillpre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import ListTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style= 'border: 1px solid black;''><tr><td align='left' style='border: .5px solid gray;''>name</td><td align='left' style='border: .5px solid gray;''>similarity</td><td align='left' style='border: .5px solid gray;''>code</td></tr><tr><td align='left' style='border: .5px solid gray;''>vanessapre</td><td align='left' style='border: .5px solid gray;''>0.446</td><td align='left' style='border: .5px solid gray;''>cf</td></tr><tr><td align='left' style='border: .5px solid gray;''>-vanessapost</td><td align='left' style='border: .5px solid gray;''>0.352</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>-williampost</td><td align='left' style='border: .5px solid gray;''>0.509</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>lesliepre</td><td align='left' style='border: .5px solid gray;''>0.481</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>zeldapre</td><td align='left' style='border: .5px solid gray;''>0.265</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>williampre</td><td align='left' style='border: .5px solid gray;''>0.421</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>-jacobpost</td><td align='left' style='border: .5px solid gray;''>0.366</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>-marthapost</td><td align='left' style='border: .5px solid gray;''>0.226</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>-robbiepost</td><td align='left' style='border: .5px solid gray;''>0.161</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>kurtpre</td><td align='left' style='border: .5px solid gray;''>0.233</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>angelapre</td><td align='left' style='border: .5px solid gray;''>1.0</td><td align='left' style='border: .5px solid gray;''>cf</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftcandice</td><td align='left' style='border: .5px solid gray;''>0.46</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>-bethpost</td><td align='left' style='border: .5px solid gray;''>0.101</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftharmony</td><td align='left' style='border: .5px solid gray;''>0.578</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>-alipost</td><td align='left' style='border: .5px solid gray;''>0.434</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftcaitlin</td><td align='left' style='border: .5px solid gray;''>0.451</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>denisepre</td><td align='left' style='border: .5px solid gray;''>0.352</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>randypre</td><td align='left' style='border: .5px solid gray;''>0.556</td><td align='left' style='border: .5px solid gray;''>cf</td></tr><tr><td align='left' style='border: .5px solid gray;''>ovadyapre</td><td align='left' style='border: .5px solid gray;''>0.378</td><td align='left' style='border: .5px solid gray;''>cf</td></tr><tr><td align='left' style='border: .5px solid gray;''>-kurtpost</td><td align='left' style='border: .5px solid gray;''>0.384</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftblake</td><td align='left' style='border: .5px solid gray;''>0.364</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>alipre</td><td align='left' style='border: .5px solid gray;''>0.386</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>jacobpre</td><td align='left' style='border: .5px solid gray;''>0.447</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftsamantha</td><td align='left' style='border: .5px solid gray;''>0.54</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>kimberleypre</td><td align='left' style='border: .5px solid gray;''>0.301</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>-ovadyapost</td><td align='left' style='border: .5px solid gray;''>0.236</td><td align='left' style='border: .5px solid gray;''>cf</td></tr><tr><td align='left' style='border: .5px solid gray;''>-stanpost</td><td align='left' style='border: .5px solid gray;''>0.333</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftcassandra</td><td align='left' style='border: .5px solid gray;''>0.488</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftlibby</td><td align='left' style='border: .5px solid gray;''>0.545</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>-randypost</td><td align='left' style='border: .5px solid gray;''>0.167</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>-sandrapost</td><td align='left' style='border: .5px solid gray;''>0.378</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftlisa</td><td align='left' style='border: .5px solid gray;''>0.368</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftmarcus</td><td align='left' style='border: .5px solid gray;''>0.578</td><td align='left' style='border: .5px solid gray;''>cf</td></tr><tr><td align='left' style='border: .5px solid gray;''>kellypre</td><td align='left' style='border: .5px solid gray;''>0.462</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>deidrapre</td><td align='left' style='border: .5px solid gray;''>0.481</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftalex</td><td align='left' style='border: .5px solid gray;''>0.466</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftelesha</td><td align='left' style='border: .5px solid gray;''>0.347</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>bethpre</td><td align='left' style='border: .5px solid gray;''>0.451</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>-amandapost</td><td align='left' style='border: .5px solid gray;''>0.072</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>jillpre</td><td align='left' style='border: .5px solid gray;''>0.399</td><td align='left' style='border: .5px solid gray;''>cf</td></tr><tr><td align='left' style='border: .5px solid gray;''>-kimberleypost</td><td align='left' style='border: .5px solid gray;''>0.421</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>-denisepost</td><td align='left' style='border: .5px solid gray;''>0.462</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>robbiepre</td><td align='left' style='border: .5px solid gray;''>0.425</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>edgarpre</td><td align='left' style='border: .5px solid gray;''>0.323</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftmason</td><td align='left' style='border: .5px solid gray;''>0.434</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftholly</td><td align='left' style='border: .5px solid gray;''>0.569</td><td align='left' style='border: .5px solid gray;''>side</td></tr><tr><td align='left' style='border: .5px solid gray;''>markpre</td><td align='left' style='border: .5px solid gray;''>0.419</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>amandapre</td><td align='left' style='border: .5px solid gray;''>0.23</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>-edgarpost</td><td align='left' style='border: .5px solid gray;''>0.199</td><td align='left' style='border: .5px solid gray;''>cf</td></tr><tr><td align='left' style='border: .5px solid gray;''>richardpre</td><td align='left' style='border: .5px solid gray;''>0.337</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>-markpost</td><td align='left' style='border: .5px solid gray;''>0.333</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftkerri</td><td align='left' style='border: .5px solid gray;''>0.496</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftsophia</td><td align='left' style='border: .5px solid gray;''>0.309</td><td align='left' style='border: .5px solid gray;''>none</td></tr><tr><td align='left' style='border: .5px solid gray;''>ftcaden</td><td align='left' style='border: .5px solid gray;''>0.261</td><td align='left' style='border: .5px solid gray;''>tilt</td></tr></table>"
      ],
      "text/plain": [
       "[['name', 'similarity', 'code'],\n",
       " ['vanessapre', '0.446', 'cf'],\n",
       " ['-vanessapost', '0.352', 'none'],\n",
       " ['-williampost', '0.509', 'none'],\n",
       " ['lesliepre', '0.481', 'side'],\n",
       " ['zeldapre', '0.265', 'tilt'],\n",
       " ['williampre', '0.421', 'none'],\n",
       " ['-jacobpost', '0.366', 'none'],\n",
       " ['-marthapost', '0.226', 'tilt'],\n",
       " ['-robbiepost', '0.161', 'side'],\n",
       " ['kurtpre', '0.233', 'side'],\n",
       " ['angelapre', '1.0', 'cf'],\n",
       " ['ftcandice', '0.46', 'none'],\n",
       " ['-bethpost', '0.101', 'tilt'],\n",
       " ['ftharmony', '0.578', 'none'],\n",
       " ['-alipost', '0.434', 'none'],\n",
       " ['ftcaitlin', '0.451', 'tilt'],\n",
       " ['denisepre', '0.352', 'side'],\n",
       " ['randypre', '0.556', 'cf'],\n",
       " ['ovadyapre', '0.378', 'cf'],\n",
       " ['-kurtpost', '0.384', 'none'],\n",
       " ['ftblake', '0.364', 'tilt'],\n",
       " ['alipre', '0.386', 'side'],\n",
       " ['jacobpre', '0.447', 'side'],\n",
       " ['ftsamantha', '0.54', 'side'],\n",
       " ['kimberleypre', '0.301', 'none'],\n",
       " ['-ovadyapost', '0.236', 'cf'],\n",
       " ['-stanpost', '0.333', 'tilt'],\n",
       " ['ftcassandra', '0.488', 'side'],\n",
       " ['ftlibby', '0.545', 'side'],\n",
       " ['-randypost', '0.167', 'side'],\n",
       " ['-sandrapost', '0.378', 'tilt'],\n",
       " ['ftlisa', '0.368', 'tilt'],\n",
       " ['ftmarcus', '0.578', 'cf'],\n",
       " ['kellypre', '0.462', 'none'],\n",
       " ['deidrapre', '0.481', 'side'],\n",
       " ['ftalex', '0.466', 'none'],\n",
       " ['ftelesha', '0.347', 'tilt'],\n",
       " ['bethpre', '0.451', 'tilt'],\n",
       " ['-amandapost', '0.072', 'tilt'],\n",
       " ['jillpre', '0.399', 'cf'],\n",
       " ['-kimberleypost', '0.421', 'none'],\n",
       " ['-denisepost', '0.462', 'side'],\n",
       " ['robbiepre', '0.425', 'side'],\n",
       " ['edgarpre', '0.323', 'none'],\n",
       " ['ftmason', '0.434', 'none'],\n",
       " ['ftholly', '0.569', 'side'],\n",
       " ['markpre', '0.419', 'tilt'],\n",
       " ['amandapre', '0.23', 'tilt'],\n",
       " ['-edgarpost', '0.199', 'cf'],\n",
       " ['richardpre', '0.337', 'none'],\n",
       " ['-markpost', '0.333', 'tilt'],\n",
       " ['ftkerri', '0.496', 'none'],\n",
       " ['ftsophia', '0.309', 'none'],\n",
       " ['ftcaden', '0.261', 'tilt']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab = ListTable()\n",
    "tab.append([\"name\", \"similarity\", \"code\"])\n",
    "for name in doc_vectors.keys():\n",
    "    tab.append([name, str(compare_students(name, 'angelapre')), seasons_corpus[name][1]])\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare to pre-written comparison documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seasons_module import load_seasons_comparison_files\n",
    "comparison_dict = load_seasons_comparison_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute vectors for the comparison documents\n",
    "comparison_vectors = {}\n",
    "for fname in comparison_dict.keys():\n",
    "    comparison_vectors[fname] = norm_vec(np.array([comparison_dict[fname].count(word) for word in pruned_vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_to_compvecs(s1):\n",
    "    resultdict = {}\n",
    "    for cname in comparison_vectors.keys():\n",
    "        resultdict[cname] = dot(doc_vectors[s1], comparison_vectors[cname])\n",
    "    return resultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'side': 0.5143281672291301,\n",
       " 'tilt': 0.39569872439840387,\n",
       " 'cf': 0.5837300238472753}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_to_compvecs(\"angelapre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def max_from_dict(the_dict):\n",
    "    key, value = max(the_dict.items(), key=lambda x:x[1])\n",
    "    return key\n",
    "\n",
    "student_codes = {}\n",
    "for name in doc_vectors.keys():\n",
    "    student_codes[name] = max_from_dict(compare_to_compvecs(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How similar are our results to the codes assigned by human coders?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_accuracy():\n",
    "    number_right = 0\n",
    "    total_possible = 0\n",
    "    for name in student_codes.keys():\n",
    "        if seasons_corpus[name][1] != \"none\":\n",
    "            total_possible += 1\n",
    "            if student_codes[name] == seasons_corpus[name][1]:\n",
    "                number_right += 1\n",
    "    return 1.0 * number_right / total_possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gold_list = []\n",
    "test_list = []\n",
    "for name in student_codes.keys():\n",
    "    if seasons_corpus[name][1] != \"none\":\n",
    "        gold_list += [seasons_corpus[name][1]]\n",
    "        test_list += [student_codes[name]]\n",
    "cm = nltk.ConfusionMatrix(gold_list, test_list)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some slightly different ways of computing document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First variant: use just a subset of the vocabulary when constructing the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fdist = nltk.FreqDist()\n",
    "for fname in seasons_corpus.keys():\n",
    "    pruned_transcript_words = [w for w in seasons_corpus[fname][0] if w not in stop_list]\n",
    "    word_fdist.update(pruned_transcript_words)\n",
    "word_fdist.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = [w[0] for w in word_fdist.most_common(50) if w not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the document vector for each document\n",
    "doc_vectors = {}\n",
    "for fname in seasons_corpus.keys():\n",
    "    doc_vectors[fname] = norm_vec(np.array([seasons_corpus[fname][0].count(word) for word in new_vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute vectors for the comparison documents\n",
    "comparison_vectors = {}\n",
    "for fname in comparison_dict.keys():\n",
    "    comparison_vectors[fname] = norm_vec(np.array([comparison_dict[fname].count(word) for word in new_vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_codes = {}\n",
    "for name in doc_vectors.keys():\n",
    "    student_codes[name] = max_from_dict(compare_to_compvecs(name))\n",
    "compute_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list = []\n",
    "test_list = []\n",
    "for name in student_codes.keys():\n",
    "    if seasons_corpus[name][1] != \"none\":\n",
    "        gold_list += [seasons_corpus[name][1]]\n",
    "        test_list += [student_codes[name]]\n",
    "cm = nltk.ConfusionMatrix(gold_list, test_list)\n",
    "cm\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other variants: Use different weight factors when constructing the vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A weight factor function will commonly use these different quantities in some combination\n",
    "\n",
    "* `tf = term frequency` (number of times the term appears in the present document)\n",
    "* `df = document frequency` (number of documents in which the term appears)\n",
    "* `cf = corpus frequency` (total number of times the term appears in the entire corpus)\n",
    "* `N = number of documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(tf, df, cf, N):\n",
    "    return tf\n",
    "\n",
    "def logtf(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf))\n",
    "    return result\n",
    "\n",
    "def onehot(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def tfidf(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf)) * np.log(N  / df)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to construct the document frequency distribution since we don't have that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_fdist = nltk.FreqDist()\n",
    "for fname in seasons_corpus.keys():\n",
    "    pruned_transcript_words = [w for w in seasons_corpus[fname][0] if w not in stop_list]\n",
    "    doc_fdist.update(list(set(pruned_transcript_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little function to simplify the task of constructing vectors with different weight factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vector(words, vocab, df, cf, N, weight_function):\n",
    "    new_vector = []\n",
    "    for w in vocab:\n",
    "        tf = words.count(w)\n",
    "        new_vector.append(weight_function(tf, df[w], cf[w], N))\n",
    "    return norm_vec(np.array(new_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the document vector for each document\n",
    "doc_vectors = {}\n",
    "N = len(seasons_corpus.keys())\n",
    "wf = tfidf\n",
    "for fname in seasons_corpus.keys():\n",
    "    doc_vectors[fname] = compute_vector(seasons_corpus[fname][0], new_vocab, doc_fdist, word_fdist, N, wf)\n",
    "# Compute vectors for the comparison documents\n",
    "comparison_vectors = {}\n",
    "for fname in comparison_dict.keys():\n",
    "    comparison_vectors[fname] = compute_vector(comparison_dict[fname], new_vocab, doc_fdist, word_fdist, N, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_codes = {}\n",
    "for name in doc_vectors.keys():\n",
    "    student_codes[name] = max_from_dict(compare_to_compvecs(name))\n",
    "compute_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
