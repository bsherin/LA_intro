{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aerial-referral",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-return",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def norm_vec(v):\n",
    "    mag = np.linalg.norm(v)\n",
    "    if mag == 0:\n",
    "        return v\n",
    "    return v / np.linalg.norm(v)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def normalize_rows(x):\n",
    "    return normalize(x, axis=1)\n",
    "\n",
    "def normalize_columns(x):\n",
    "    return normalize(x, axis=0)\n",
    "\n",
    "def check_float(potential_float):\n",
    "    try:\n",
    "        float(potential_float)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def round_if_float(v, prec=3):\n",
    "    if check_float(v):\n",
    "        return round(float(v), prec)\n",
    "    return v\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "def list_table(the_list, color_nums=False):\n",
    "    html = [\"<table style= 'border: 1px solid black; display:inline-block'>\"]\n",
    "    for row in the_list:\n",
    "        html.append(\"<tr>\")\n",
    "        for col in row:\n",
    "            if color_nums and check_float(col) and not float(col) == 0:\n",
    "                html.append(\"<td align='left' style='border: .5px solid gray; color: {1}; font-weight: bold'>{0}</td>\".format(round_if_float(col), color_nums))\n",
    "            else:\n",
    "                html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(round_if_float(col)))\n",
    "        html.append(\"</tr>\")\n",
    "    html.append(\"</table>\")\n",
    "    return display(HTML(''.join(html)))\n",
    "\n",
    "def show_labeled_table(mat, col_names=None, row_names=None, nrows=10, ncols=10, color_nums=\"red\"):\n",
    "    sml = mat[:nrows, :ncols]\n",
    "    if col_names is not None:\n",
    "        sml = np.vstack([col_names[:ncols], sml])\n",
    "    if row_names is not None:\n",
    "        rnames = [[p] for p in row_names[:nrows]]\n",
    "        if col_names is not None:\n",
    "            new_col = np.array([[\"_\"]] + rnames)\n",
    "        else:\n",
    "            new_col = np.array(rnames)\n",
    "        sml = np.hstack((new_col, sml))\n",
    "    return list_table(sml, color_nums)\n",
    "\n",
    "def compute_doc_vector(tdoc, vocab):\n",
    "    return np.array([tdoc.count(w) for w in vocab])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monetary-browser",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load, prepare the training corpus as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "fname = 'corpora/seasons_training.txt'\n",
    "f = open(fname)\n",
    "raw = f.read().lower()\n",
    "whole_training_docs = re.findall(r\"<text>([\\s\\S]*?)</text>\", raw)\n",
    "\n",
    "training_docs = []\n",
    "para_names = []\n",
    "for i, d in enumerate(whole_training_docs):\n",
    "    new_docs = nltk.sent_tokenize(d)\n",
    "    training_docs += new_docs\n",
    "    new_names = [\"d{}p{}\".format(i, p) for p in range(len(new_docs))]\n",
    "    para_names += new_names\n",
    "    \n",
    "len(training_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-carbon",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seasons_module import seasons_tokenize\n",
    "tokenized_training_docs = []\n",
    "f = open(\"lists/seasons_stop_list.txt\")\n",
    "stop_list = set(f.read().split(\"\\n\"))\n",
    "for doc in training_docs:\n",
    "    tdoc = seasons_tokenize(doc)\n",
    "    tdoc = [w for w in tdoc if w not in stop_list]\n",
    "    tokenized_training_docs.append(tdoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-throw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec is an algorithm that will take tokenized sentences and produces a set of word vectors for us.\n",
    "\n",
    "We'll use the implementation in the gensim library ([docs](https://radimrehurek.com/gensim/models/word2vec.html))\n",
    "\n",
    "There are a few online pages that go into a bit of detail explaining word2vec. (For [example](https://jalammar.github.io/illustrated-word2vec/).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-winning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "wv_model = Word2Vec(sentences=tokenized_training_docs,\n",
    "                    vector_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-springer",
   "metadata": {},
   "source": [
    "This produces what gensim calls a [keyed vector](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors) object that we can use to, among other things, get a vector for a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "kvecs = wv_model.wv\n",
    "kvecs[\"side\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-cherry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_words(w1, w2, wvecs):\n",
    "    return wvecs.similarity(w1, w2)\n",
    "\n",
    "def get_word_vector(w1, wvecs):\n",
    "    return norm_vec(wvecs[w1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_words(\"side\", \"side\", kvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_words(\"northern\", \"hemisphere\", kvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_words(\"close\", \"hemisphere\", kvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-upgrade",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_table(kvecs.similar_by_key(\"hemisphere\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_table(kvecs.similar_by_key('tilt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def squish_matrix(X, ncomponents=2):\n",
    "    pca = PCA(n_components=ncomponents)\n",
    "    return pca.fit_transform(X)\n",
    "\n",
    "def double_squish(X, ncomponents=2):\n",
    "    pca = PCA(n_components=50)\n",
    "    reduc = pca.fit_transform(X)\n",
    "    return TSNE(n_components=ncomponents, random_state=0, perplexity=15).fit_transform(reduc)\n",
    "\n",
    "\n",
    "def alt_squish(X, ncomponents=2):\n",
    "    return TSNE(n_components=ncomponents, random_state=0, perplexity=15).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-thesaurus",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_matrix(mat, labels, number_to_plot=10, figsize=(10, 10), c=\"red\"):\n",
    "    if mat.shape[1] > 3:\n",
    "        print(\"too many dimensions\")\n",
    "        return\n",
    "    if number_to_plot > mat.shape[0]:\n",
    "        number_to_plot = mat.shape[0]\n",
    "    fig=plt.figure(figsize=figsize, dpi= 80, facecolor='w', edgecolor='k')\n",
    "    if mat.shape[1] == 3:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(mat[:, 0][:number_to_plot], mat[:, 1][:number_to_plot], mat[:, 2][:number_to_plot], c=c)\n",
    "        for i in range(number_to_plot):\n",
    "            ax.text(mat[i, 0], mat[i, 1], mat[i, 2], labels[i])\n",
    "\n",
    "    else:\n",
    "        plt.scatter(mat[:, 0][:number_to_plot], mat[:, 1][:number_to_plot], c=c)\n",
    "        for i in range(number_to_plot):\n",
    "            plt.annotate(labels[i], mat[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import copy, random\n",
    "def plot_similar(the_word, kvecs, dims=2, n=10):\n",
    "    similar_words = [w[0] for w in kvecs.similar_by_key(the_word, 1000)]\n",
    "    folded_vocab_list = [kvecs[the_word]]\n",
    "    found_vocab = [the_word]\n",
    "    colors = [\"red\"]\n",
    "    for w in similar_words[:n]:\n",
    "        found_vocab.append(w)\n",
    "        v = kvecs[w]\n",
    "        folded_vocab_list.append(v)\n",
    "        colors.append(\"blue\")\n",
    "    random.shuffle(similar_words)\n",
    "    for w in similar_words[:n]:\n",
    "        found_vocab.append(w)\n",
    "        v = kvecs[w]\n",
    "        folded_vocab_list.append(v)\n",
    "        colors.append(\"green\")\n",
    "    folded_vocab_matrix = np.array(folded_vocab_list)\n",
    "    squished_matrix = alt_squish(folded_vocab_matrix, dims)\n",
    "    plot_matrix(squished_matrix, found_vocab, 50, c=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similar(\"hemisphere\", kvecs, dims=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-basin",
   "metadata": {},
   "source": [
    "`a : b :: c : d` and we have to find the word ‘d’. \n",
    "\n",
    "The associated word vectors va, vb, vc, vd are related to each other in the following relationship: `vb – va = vd – vc`\n",
    "\n",
    "Example: `Paris:France::Berlin::Germany`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = word_vectors.most_similar(positive=['France', 'Berlin'], negative=['Paris'])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "316.59375px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
