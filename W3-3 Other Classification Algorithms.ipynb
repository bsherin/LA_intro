{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## This is the same procedures we defined in the last notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def prepare_feature_sets(pass_feature_func):\n",
    "    csvfile = open(\"corpora/titanic/train.csv\")\n",
    "    dlist = list(csv.DictReader(csvfile))\n",
    "    random.shuffle(dlist)\n",
    "    labeled_feature_sets = [(pass_feature_func(r), r[\"Survived\"]) for r in dlist]\n",
    "    return labeled_feature_sets\n",
    "\n",
    "def test_classifier(train_set, test_set):\n",
    "    classif = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    accuracy = nltk.classify.accuracy(classif, test_set)\n",
    "    gold_list = [t[1] for t in test_set]\n",
    "    test_list = [classif.classify(t[0]) for t in test_set]\n",
    "    kappa = cohen_kappa_score(gold_list, test_list)\n",
    "    return {\"accuracy\": accuracy, \"kappa\": kappa}\n",
    "\n",
    "def cross_validate(nchunks, labeled_featuresets):\n",
    "    test_fraction = 1 / nchunks\n",
    "    test_size = round(test_fraction * len(labeled_featuresets))\n",
    "    chunk_starts = [n * test_size for n in range(nchunks)]\n",
    "    random.shuffle(labeled_featuresets)\n",
    "    data_chunks = [labeled_featuresets[start:start+test_size] for start in chunk_starts]\n",
    "    test_set = None\n",
    "    train_set = []\n",
    "    results = []\n",
    "    for n in range(nchunks):\n",
    "        for m, c in enumerate(data_chunks):\n",
    "            if m == n:\n",
    "                test_set = c\n",
    "            else:\n",
    "                train_set += c\n",
    "        res = test_classifier(train_set, test_set)\n",
    "        results.append(res)\n",
    "    average_accuracy = sum([res[\"accuracy\"] for res in results]) / len(results)\n",
    "    average_kappa = sum([res[\"kappa\"] for res in results]) / len(results)\n",
    "    print(\"average accuracy: {}, average kappa: {}\".format(round(average_accuracy, 3), round(average_kappa, 3)))\n",
    "    \n",
    "    return results, average_accuracy, average_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passenger_features(r):\n",
    "    return {\"sex\": r[\"Sex\"], \"pclass\": r[\"Pclass\"], \"embarked\": r[\"Embarked\"]}\n",
    "\n",
    "results, average_accuracy, average_kappa = cross_validate(10, prepare_feature_sets(passenger_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification algorithms\n",
    "\n",
    "In general, there are two ways we can make our classifiers better. One is with \"feature engineering,\" by doing better at the feature sets that we feed into an algorithm. The second is by choosing the right algorithm. The Naive Bayes algorithm is just one of the many algorithms that exist. \n",
    "\n",
    "Once we have set up the machinery above, using a different algorithm can often be very easy. To use the decision tree algorithm we just have to change the first line in our `test_classifier` procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(train_set, test_set):\n",
    "    classif = nltk.DecisionTreeClassifier.train(train_set)  # This is key line\n",
    "    print(classif.pseudocode(depth=4))\n",
    "    accuracy = nltk.classify.accuracy(classif, test_set)\n",
    "    gold_list = [t[1] for t in test_set]\n",
    "    test_list = [classif.classify(t[0]) for t in test_set]\n",
    "    kappa = cohen_kappa_score(gold_list, test_list)\n",
    "    return {\"accuracy\": accuracy, \"kappa\": kappa}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passenger_features(r):\n",
    "    return {\"sex\": r[\"Sex\"], \"pclass\": r[\"Pclass\"], \"embarked\": r[\"Embarked\"]}\n",
    "\n",
    "results, average_accuracy, average_kappa = cross_validate(10, prepare_feature_sets(passenger_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Classifiers\n",
    "\n",
    "NLTK has a limited number of classification algorithms built in. But it has functionality that\n",
    "makes it relatively easy to use classifiers from other libraries. \n",
    "\n",
    "In particular, it can connect to [scikit-learn](https://scikit-learn.org/stable/supervised_learning.html), often abbreviated *sklearn*.\n",
    "\n",
    "Here we'll use the **Support Vector Machine** algorithm from sklearn. Again, this requires changing the first line in `test_classifer`. We also have to import the needed machinery from NLTK and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def test_classifier(train_set, test_set):\n",
    "    classif = SklearnClassifier(SVC(), sparse=False).train(train_set)\n",
    "    accuracy = nltk.classify.accuracy(classif, test_set)\n",
    "    gold_list = [t[1] for t in test_set]\n",
    "    test_list = [classif.classify(t[0]) for t in test_set]\n",
    "    kappa = cohen_kappa_score(gold_list, test_list)\n",
    "    return {\"accuracy\": accuracy, \"kappa\": kappa}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passenger_features(r):\n",
    "    return {\"sex\": r[\"Sex\"], \"pclass\": r[\"Pclass\"], \"embarked\": r[\"Embarked\"]}\n",
    "\n",
    "results, average_accuracy, average_kappa = cross_validate(10, prepare_feature_sets(passenger_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
